---
title: "PREDICTIVE ANALYTICS/MACHINE LEARNING"
subtitle: "BAN400"
author: "HÃ¥kon Otneim"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [default, default-fonts]
    nature:
      highlightStyle: github
      highlightLines: true
      slideNumberFormat: " "
      titleSlideClass: [middle, left, inverse]
---



## The plan for today

- This course mainly concerns the *programming* aspect of data science. In other words, we deal with a lot of the infrastructure that is necessary to perform data science in practice. 
- Today, however, we will briefly visit a core topic: namely the statistical modeling that is used to produce *predictions* in data science. This is often labelled *machine learning* or *statistical learning* (and sometimes more vaguely as AI and other fancy terms...).
- We will only give a superficial treatment today (for completeness). The topic is covered in detail in [BAN404 - Predictive Analytics with R](https://www.nhh.no/en/courses/predictive-analytics-with-r/), due to run in the spring.


### Literature

- The standard basic reference to statistical learning (and textbook in BAN404) is [ISL](https://www.statlearning.com/).
- [ESL](https://web.stanford.edu/~hastie/ElemStatLearn/) is a more advanced book.
- Both of these are freely available online.

---

## The basic (generalized) linear model 

Let us go back to basics and model the association between a set of explanatory variables (covariates, independent variables, right-hand-side-variables, $X$'s, etc ...) and a response variable (dependent variable, left-hand-side-variable, or just simply ...) $Y$ using the following linear relationship:

$$Y = \beta_0 + \beta_1X_1 + \cdots + \beta_pX_p + \epsilon,$$
where $\epsilon$ is a random variable typically assumed to be normally distributed. If $Y$ is a continuous variable we call this a *regression problem*.

If $Y$ is a binary (dummy, zero-one) variable, we use the logistic regression:

$$\textrm{logit}(P(Y = 1|X_1,\ldots,X_p)) = \beta_0 + \beta_1X_1 + \cdots + \beta_pX_p.$$
We call this a *classification problem*. 

- In classical econometrics we are often interested in statistical and causal inference.
- In modern data science we are more interested in predictions, but this distinction should not be taken too far.
- We will focus on the classification problem in the examples today, because it is easier to visualize.

---

## The rise of the black box analogy and why we don't like it

```{r, echo = FALSE, out.width="680", fig.align='center'}
knitr::include_graphics("blackbox.jpg")
```

---

## Classical example: Customer churn


```{r, message = FALSE, echo = FALSE}
library(dplyr)
telco <- readr::read_csv("telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv") %>% 
  select(Churn, MonthlyCharges, tenure) %>% 
  mutate(Churn = as.factor(Churn))
```

- We have access to a database of current and previous subscribers at a large American cell phone company (Telco).
- [Data source at Kaggle.](https://www.kaggle.com/blastchar/telco-customer-churn)
- Number of observations: $n = `r nrow(telco)`$.
- We know the length of the customer relationship, how much the customer pays each month, and wheter the customer has ended the subscription (churn).
    - The classical inference question: to what degree do the two variables explain the probability of churn?
    - The ML-question: How can we classify current customers as either loyal or at risk of ending their subscription? Who do we pick out for targeted marketing?

---

## The churn data: What do we see?

```{r, echo = FALSE, message = FALSE, warning = FALSE, fig.height = 7, fig.width = 10, fig.retina = 3, fig.align='center'}
library(dplyr)
library(ggplot2)
library(tidymodels)      # <- This is new. Tidyverse take on modeling

no_color <- "#00000020"
yes_color <- "#FF000070"
pred_color <- c("#FFFFFF00", "#AA3333DD")
cutoff <- .5
grid_resolution <- 50

# Read the data
telco <- 
  readr::read_csv("telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv") %>% 
  select(Churn, MonthlyCharges, tenure) %>% 
  mutate(Churn = as.factor(Churn))

# Make a basic scatterplot 
scatter <- 
  telco %>% 
  ggplot(aes(x = MonthlyCharges, y = tenure, colour = Churn)) +
  geom_point() +
  xlab("Monthly charges ($)") +
  ylab("Tenure (months)") +
  scale_color_manual(values=c(no_color, yes_color)) +
  theme_classic()

scatter
```

---

## Logistic regression decision boundary

```{r, echo = FALSE, message = FALSE, warning = FALSE, fig.height = 7, fig.width = 10, fig.retina = 3, fig.align='center'}
# Fit a logistic regression to the data using tidymodels
fitted_logistic <-
  logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification") %>% 
  fit(Churn ~ ., data = telco)

# Draw the decision boundary for logistic regression:
newdata <- 
  expand.grid(MonthlyCharges = seq(from = 20, 
                                   to = 118, 
                                   length.out = grid_resolution),
            tenure = seq(from = 0, 
                         to = 70, 
                         length.out = grid_resolution))

predictions <- 
  newdata %>% 
  bind_cols(pred_prob_logistic = predict(fitted_logistic, 
                                         new_data = newdata, 
                                         type = "prob")$.pred_Yes)

scatter +
  geom_contour_filled(aes(x = MonthlyCharges, 
                          y = tenure, 
                          z = pred_prob_logistic),
                      data = predictions,
                      inherit.aes = FALSE,
                      breaks = c(0, 0.5, 1)) +
  scale_fill_manual(values = pred_color, 
                    name="Predicted probability", 
                    drop = FALSE) +
  guides(fill = FALSE) +
  geom_contour(aes(x = MonthlyCharges, 
                   y = tenure, 
                   z = pred_prob_logistic),
               data = predictions,
               inherit.aes = FALSE,
               breaks = c(0, 0.5, 1),
               colour = "black",
               size = 1.5)
```

---

## Pros and cons of using LR for classification

- The logistic regression is a classical econometric model designed for *statistical inference*, i.e. figuring out which explanatory variables contributes to variation in the "success probability" $P(Y = 1)$, and in which direction.
- The logistic regression defines a proper probability model:
\begin{equation}
\log\left(\frac{P(Y = 1)}{1 - P(Y=1)}\right) = \beta_0 + \beta_1X_1 + \cdots + \beta_pX_p.
\end{equation}
- This is classical parametric statistics. Unless you introduce polynomial terms, the LR *always* leads to a straight line decision boundary.
- the LR is interpretable, requires less data (because we have decided the shape of the boundary even before we see any data).
- But if we
    1. don't want to assume that this probability model is (approximately) true, and
    2. are more interested in predicting the status of new customers than interpreting the estimated model,
    3. (and have a reasonable amount of data),
- we might want to do something else!

---

## A simple, yet powerful idea

- Make predictions based on *similarity*.
- That is: when predicting the status of a new customer $A$, look at your data set of known outcomes, and specifically the customers the are most *similar* to $A$.
    - If the customers that are most similar to $A$ tend to churn a lot, then predict $A$ to be a churner as well. 
    - If the customers that are most similar to $A$ do not tend to churn, then predict $A$ to be a safe customer.
- This is basically the starting point of the relatively recent popularity of (data science/predictive analytics/machine learning/deep learning/statistical learning/AI/...), choose your own poison.
- Today we will look at some basic principles of this idea, and point to some fallacies.

---

## A simple algorithm: kNN, k nearest neighbours

```{r, echo = FALSE, out.width="680", fig.align='center'}
knitr::include_graphics("knn.jpg")
```

$k$ nearest neighbours algorithm for predicting the class of a new individual:

- Select an integer $k$.

---

## Model complexity and the bias-variance trade-off

---

## Choosing the right complexity, test, train, x-val

---

## Measuring performance

---

## This is not magic. If you stop thinking and ignore the importance of data validity and subject field expertise, you always loose.
 - Data mining is still a dodgy activity.

---

## Other types of models

- Lasso, ridge, tree-based, neural nets

---

## Tidymodels, churn revisited




  


